import os
import pickle
import cv2
from scipy.linalg import inv
from copy import copy
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from progressbar import ProgressBar
from MatchingAlgorithm import OrbMatcher
from SessionGenerator import Bundle
import pandas as pd

class Benchmarker:
    """
    Validates the matches of user specified matching algorithms.

    Using data about the query and train images (stored as a bundle inside a session), it is
    possible to map a keypoint from the query image to a keypoint on the train
    image (and vice versa). First, we let the matching algorithm produce a list
    of matches which contain keypoints in the query and train images. Using data
    about the query and train images from the bundle, we map every keypoint
    (generated by the matching algorithm) from the query image to the train image.
    Finally, by noting how far the train keypoints produced by the matching algorithm
    are from our correctly mapped keypoints, we can score each algorithm based on how
    many of its keypoints fell within a certain distance threshold from the correctly
    mapped keypoints. A better score means that more of the keypoints generated by the
    algorithm in the train image were close to the keypoints in the train image that
    we generated based off of the bundle data.

    Instance Attributes:
        sessions (list): A list of Session objects.
        Algorithms (list): A list of MatchingAlgorithm objects to be benchmarked.
    """

    def __init__(self, algorithms, values):
        self.sessions = self.get_sessions()
        self.algorithms = algorithms
        self.sweep_values = values

    def get_sessions(self):
        """
        Loads the Session objects from a pickle file.

        Returns: A list of Session objects.
        """
        sessions_path = (
            f"{os.path.dirname(os.path.dirname(__file__))}/session_data/sessions.pkl"
        )
        with open(sessions_path, "rb") as sessions_file:
            sessions = pickle.load(sessions_file)
        return sessions
        
    def project_depth_onto_image(self, query_depth_feature_points, focal_length, offset_x, offset_y):
        """
        Projects depth map onto image using camera intrinsics.

        Args:
            query_depth_feature_points (numpy.ndarray): A numpy array 
                containing the depth map's feature points.
            focal_length (float): The focal length of the camera.
            offset_x (float): The x offset of the camera.
            offset_y (float): The y offset of the camera.
        
        Returns: A numpy array containing the projected feature points.
        """
        pixels = []
        for row in query_depth_feature_points:
            pixel_x = row[0] * focal_length / row[2] + offset_x
            pixel_y = row[1] * focal_length / row[2] + offset_y
            pixels.append((pixel_x, pixel_y))
        return pixels    

    def plot_depth_map(self, bundle, pixels, image):
        for i, pixel in enumerate(pixels):
            output = cv2.circle(
                image,
                (int(pixel[0]), int(pixel[1])),
                1,
                (
                    bundle.query_image_depth_map[i][3] * 255,
                    bundle.query_image_depth_map[i][3] * 255,
                    bundle.query_image_depth_map[i][3] * 255,
                ),
                -1,
            )
        return output

    def draw_circle(self, image, keypoint, color):
        return cv2.circle(
                image,
                (keypoint[0], keypoint[1]),
                20,
                color,
                -1,
            )
    
    def svd_rotation(self, query, train):
        """
        Calculates the rotation matrix that rotates A onto B.
        """
        # calc center of mass or mean of data
        com_query = np.mean(query, axis=1)
        com_train = np.mean(train, axis=1)
        query_centered = np.array([np.subtract(row, com_query) for row in query.T])
        train_centered = np.array([np.subtract(row, com_train) for row in train.T])
        W = query_centered.T @ train_centered  
        U, S, V_t = np.linalg.svd(W)
        R = np.dot(U, V_t)
        t = com_query - np.dot(R, com_train)
        # pdb.set_trace()
        return R, t

    def convert_depth_vectors(self, depth_data):
        lidar_depths = []
        for row in depth_data:
            x = row[0] * row[3]
            y = row[1] * row[3]
            z = row[2] * row[3]
            lidar_depths.append([x, y, z]) 
        depth_data = np.array(lidar_depths)
        depth_data = np.hstack(
            (depth_data, np.ones((depth_data.shape[0], 1)))
        ).T
        return depth_data

    def get_transformation(self, query_data, train_data, matches):
        """
        Returns the transformation matrix to project train data onto query data for the given bundle.
        """
        matched_query_depth_matrix = []
        matched_train_depth_matrix = []
        query_depth_data = self.convert_depth_vectors(query_data)
        train_depth_data = self.convert_depth_vectors(train_data)
        for unimatch in matches:
            matched_query_keypoint = (int(unimatch.queryPt.x), int(unimatch.queryPt.y))
            matched_train_keypoint = (int(unimatch.trainPt.x), int(unimatch.trainPt.y))

            corresponding_query_depth_index = round(
                matched_query_keypoint[0] / 7.5
            ) * 192 + round(matched_query_keypoint[1] / 7.5)
            corresponding_train_depth_index = round(
                matched_train_keypoint[0] / 7.5
            ) * 192 + round(matched_train_keypoint[1] / 7.5)

            matched_query_depth_matrix.append(np.array(query_depth_data[:,corresponding_query_depth_index]).reshape(4,1))
            matched_train_depth_matrix.append(np.array(train_depth_data[:,corresponding_train_depth_index]).reshape(4,1))
        matched_query_depth_matrix = np.concatenate(matched_query_depth_matrix, axis=1)
        matched_train_depth_matrix = np.concatenate(matched_train_depth_matrix, axis=1)
        R, t = self.svd_rotation(matched_train_depth_matrix, matched_query_depth_matrix)
        print("found rotation")
        return R,t

    def compare_matches(self, bundle, matches, query_image, train_image, crossmatch=False, R=None, t=None, count=0):
        """
        Compares the matches produced by the algorithm with the matches produced
        by the depth map data.

        Args: 
            bundle (Bundle): A Bundle object containing the data about the query and train images.
            matches (list): A list of UniMatch objects produced by the algorithm.
            query_image (numpy.ndarray): The query image.
            train_image (numpy.ndarray): The train image.

        Returns: A list of distances between the matches produced by the algorithm
        and the matches produced by the depth map data.
        """
        ## Depth map of query image
        focal_length = bundle.query_image_intrinsics[0]
        offset_x = bundle.query_image_intrinsics[6]
        offset_y = bundle.query_image_intrinsics[7]

        query_depth_data = self.convert_depth_vectors(bundle.query_image_depth_map)
        train_depth_data = self.convert_depth_vectors(bundle.train_image_depth_map)
        
        # Actual depth feature points, with magnitude removed from the vector.
        query_depth_feature_points = np.array(
            (query_depth_data[0], -query_depth_data[1], -query_depth_data[2])
        ).T

        # calculate depths and pixels of feature points
        pixels = self.project_depth_onto_image(query_depth_feature_points, focal_length, offset_x, offset_y)

        final_query_image = self.plot_depth_map(bundle, pixels, query_image)

        ## Depth map of train image

        query_pose = np.array(bundle.query_image_pose).reshape(4, 4).T
        train_pose = np.array(bundle.train_image_pose).reshape(4, 4).T

        if crossmatch:
            # instead of rotating query data to project onto train data like we do below, here we are transforming the train data relative to the query data.
            # Both ways are used to align the query and train depth data together but are slightly different.
            print("R", R)
            print("t", t)
            rotation_applied = R @ train_depth_data
            no_pose_query_depth_data_projected_on_train = np.array([np.add(row, t) for row in rotation_applied.T]).T

            # pose_difference = inv(query_pose) @ train_pose
            icp_transform = np.append(R[:3,:3],t[:3].reshape(3,1), axis=1)
            icp_transform = np.append(icp_transform, [[0,0,0,1]], axis=0)
            print(icp_transform)
            print(train_pose.shape)
            print(query_pose.shape)
            # you can recover icp_transform as: inv(query_pose) @ final_transform @ train_pose
            if count > 1:
                query_depth_data_projected_on_train = recovered_icp_transform @ train_depth_data
                recovered_icp_transform = inv(query_pose) @ final_transform @ train_pose
                final_transform = query_pose @ icp_transform @ inv(train_pose)
            else:
                query_depth_data_projected_on_train = icp_transform @ train_depth_data
                final_transform = query_pose @ icp_transform @ inv(train_pose)
                count += 1
            
        else:
            pose_difference = inv(query_pose) @ train_pose
            query_depth_data_projected_on_train = inv(pose_difference) @ query_depth_data
        projected_depth_feature_points = np.array(
            (
                query_depth_data_projected_on_train[0],
                -query_depth_data_projected_on_train[1],
                -query_depth_data_projected_on_train[2],
            )
        ).T

        pixels = []
        pixels = self.project_depth_onto_image(projected_depth_feature_points, focal_length, offset_x, offset_y)

        final_train_image = self.plot_depth_map(bundle, pixels, train_image)

        depth_point_to_algo_point_distances = []
        ## Project corresponding query image keypoints onto train image which are matched using depth map data
        for unimatch in matches:
            matched_query_keypoint = (int(unimatch.queryPt.x), int(unimatch.queryPt.y))
            matched_train_keypoint = (int(unimatch.trainPt.x), int(unimatch.trainPt.y))

            # get corresponding depth map index for each keypoint. 
                # Keypoints in a rectangular area around a depth index are matched to same index.
                # This is done since the resolution of depth map is lower than the resolution of the image.
            corresponding_depth_index = round(
                matched_query_keypoint[0] / 7.5
            ) * 192 + round(matched_query_keypoint[1] / 7.5)

            # Draw query image keypoints
            final_query_image = self.draw_circle(final_query_image, matched_query_keypoint, (0, 0, 0))

            algo_matched_point = np.array((matched_train_keypoint[0], matched_train_keypoint[1]))
            depth_matched_point = np.array((int(pixels[corresponding_depth_index][0]), int(pixels[corresponding_depth_index][1])))
            
            # Draw train image keypoints, matched using the algorithm
            final_train_image = self.draw_circle(final_train_image, algo_matched_point, (0, 0, 0))

            # Plots corresponding depth point from query image on train image, matched using the depth data
            final_train_image  = self.draw_circle(final_train_image, depth_matched_point, (255,255,255))
            
            # draw line between algo matched point and depth matched point
            final_train_image = cv2.line(
                final_train_image,
                algo_matched_point,
                depth_matched_point,
                (
                    255,
                    255,
                    255,
                ),
                1,
            )

            depth_point_to_algo_point_distances.append(np.linalg.norm(algo_matched_point - depth_matched_point))
        # cv2.imwrite("query.png", final_query_image)
        # cv2.imwrite("train.png", final_train_image)
        # d = input("d")

        return depth_point_to_algo_point_distances, final_query_image, final_train_image
    def benchmark(self):
        """
        Scores each algorithm based on how close their matches are with matches
        produced by the bundle data for each image pair.
        """
        total_points_less_than_100 = []
        total_points = []
        correct_vs_incorrect_for_one_algo = []
        total_correct_for_one_algo = []
        output_data = []
        for algorithm in self.algorithms:
            for quantile in self.sweep_values:
                for session in self.sessions:
                    print(repr(algorithm), session, quantile)
                    pbar = ProgressBar()
                    for bundle in pbar(session.bundles):
                        query_image = copy(bundle.query_image)
                        train_image = copy(bundle.train_image)

                        matches = algorithm.get_matches(query_image, train_image, quantile, ratio_not_quantile=True)
                        try:
                            distances, final_query_image, final_train_image = self.compare_matches(bundle, matches, query_image, train_image)
                            out_dict = {
                                "bundle": bundle,
                                "matches": matches,
                                "distances": distances,
                                "algorithm": algorithm,
                                "quantile": quantile,
                            }
                            out_df = pd.DataFrame(out_dict)
                            output_data.append(out_df)
                            # print("distances", len(distances))
                        except Exception as e:
                            print(e)
                        
                        filtered_points = [x for x in distances if x < 100]
                        if len(distances) > 0:
                            total_points_less_than_100.append(len(filtered_points))
                            total_points.append(len(distances))

                try:
                    correct_vs_incorrect_for_one_algo.append((quantile, sum(total_points_less_than_100) / sum(total_points), algorithm))
                    total_correct_for_one_algo.append(sum(total_points_less_than_100))
                    total_points_less_than_100 = []
                    total_points = []
                except ZeroDivisionError:
                    continue
                    # print("No matches found for", repr(algorithm), session, quantile)
                    total_points_less_than_100 = []
                    total_points = []
        
        return output_data
    
    def cross_benchmark(self, first_session, second_session, cross_algorithms, cross_quantiles):
        """
        Cross matches image pairs from different sessions.
        """
        index_matched = None
        R = None
        t = None
        # TODO: add april tags to the images for a better cross match marker.
        # TODO: Does Clew have a version with depth data collection cuz Marc lost his computer and we don't want to write a new one.
        # TODO: return some form of structured data with like matches or smth.
        # TODO: use google cloud anchors visual alignment thingy.
        # TODO: meetup with Paul to sit together and implement more data logging in the latest version of Clew with the cloud anchors and stuff.
        for i, bundle2 in enumerate(second_session.bundles):
            query_image = copy(bundle2.query_image)
            train_image = copy(bundle2.train_image)
            query_matches = OrbMatcher().get_matches(query_image, first_session.bundles[0].query_image, 0.6, ratio_not_quantile=True)
            train_matches = OrbMatcher().get_matches(train_image, first_session.bundles[0].query_image, 0.6, ratio_not_quantile=True)
            if len(query_matches) > 4:
                index_matched = i
                print("match")
                R, t = self.get_transformation(first_session.bundles[0].query_image_depth_map, bundle2.query_image_depth_map, query_matches)
                # for point in query_matches:
                #     matched_query_keypoint = (int(point.queryPt.x), int(point.queryPt.y))
                #     matched_train_keypoint = (int(point.trainPt.x), int(point.trainPt.y))
                #     out_query_image = self.draw_circle(query_image, matched_query_keypoint, (255, 255, 255))
                #     out_train_image = self.draw_circle(first_session.bundles[0].query_image, matched_train_keypoint, (255, 255, 255))
                # out = np.concatenate((out_query_image, out_train_image), axis=1)
                # cv2.imshow("out", out)
                # cv2.waitKey(0)
                break
            if len(train_matches) > 4:
                index_matched = i
                print("match")
                R, t = self.get_transformation(first_session.bundles[0].query_image_depth_map, bundle2.train_image_depth_map, train_matches)
                break
        if index_matched is None:
            print("No matches found")
            return None
        else:
            print(index_matched)
            total_points_less_than_100 = []
            total_points = []
            correct_vs_incorrect_for_one_algo = []
            for algorithm in cross_algorithms:
                for quantile in cross_quantiles:
                    pbar = ProgressBar()
                    for i, bundle2 in enumerate(pbar(second_session.bundles[index_matched:])):
                        # print(repr(algorithm), bundle2, quantile)
                        for j in range(2):
                            if j == 0:
                                query_image = copy(bundle2.query_image)
                                train_image = copy(first_session.bundles[i].query_image)
                                bundle = Bundle([
                                    first_session.bundles[i].query_image,
                                    first_session.bundles[i].query_image_depth_map,
                                    first_session.bundles[i].query_image_confidence_map,
                                    first_session.bundles[i].query_image_pose,
                                    first_session.bundles[i].query_image_intrinsics,
                                    bundle2.query_image,
                                    bundle2.query_image_depth_map,
                                    bundle2.query_image_confidence_map,
                                    bundle2.query_image_pose,
                                    bundle2.query_image_intrinsics,
                                ])
                            else:
                                query_image = copy(bundle2.train_image)
                                train_image = copy(first_session.bundles[i].train_image)
                                bundle = Bundle([
                                    first_session.bundles[i].train_image,
                                    first_session.bundles[i].train_image_depth_map,
                                    first_session.bundles[i].train_image_confidence_map,
                                    first_session.bundles[i].train_image_pose,
                                    first_session.bundles[i].train_image_intrinsics,
                                    bundle2.train_image,
                                    bundle2.train_image_depth_map,
                                    bundle2.train_image_confidence_map,
                                    bundle2.train_image_pose,
                                    bundle2.train_image_intrinsics,
                                ])
                            matches = algorithm.get_matches(query_image, train_image, quantile)
                            try:
                                distances, final_query_image, final_train_image = self.compare_matches(bundle, matches, query_image, train_image, crossmatch=True, R=R, t=t)
                            except Exception as e:
                                print(e)
                                continue
                            filtered_points = [x for x in distances if x < 100]
                            if len(distances) > 0:
                                total_points_less_than_100.append(len(filtered_points))
                                total_points.append(len(distances))

                    try:
                        correct_vs_incorrect_for_one_algo.append((quantile, sum(total_points_less_than_100) / sum(total_points), algorithm))
                        total_points_less_than_100 = []
                        total_points = []
                    except ZeroDivisionError:
                        continue
                        # print("No matches found for", repr(algorithm), session, quantile)
                        total_points_less_than_100 = []
                        total_points = []
        print("total", len(correct_vs_incorrect_for_one_algo))
        for x in correct_vs_incorrect_for_one_algo:
            if repr(x[-1]) == "Orb":
                plt.plot(x[0], x[1], 'o', color='red', label="Orb")
            elif repr(x[-1]) == "Sift":
                plt.plot(x[0], x[1], '^', color='green', label="Sift")
            elif repr(x[-1]) == "Akaze":
                plt.plot(x[0], x[1], '*', color ='blue', label="Akaze")
        plt.xticks(self.sweep_values)
        plt.xlabel("Quantile values")
        plt.ylabel("Ratio of correct/total matches")
        label_orb = mpatches.Patch(color='red', label='Orb')
        label_sift = mpatches.Patch(color='green', label='Sift')
        label_akaze = mpatches.Patch(color='blue', label='Akaze')
        plt.legend(handles=[label_orb, label_sift, label_akaze])
        plt.savefig("Ratio test cross.png")




