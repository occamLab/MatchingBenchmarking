"""
Using data about the query and train images (stored as a bundle inside a session), it is
possible to map a keypoint from the query image to a keypoint on the train
image (and vice versa). First, we let the matching algorithm produce a list
of matches which contain keypoints in the query and train images. 
Then, we use the depth map data and phone poses to map the project LIDAR keypoints in 
the query image to keypoints in the train image. This will act as our ground truth. 
We can compare the keypoints produced by the matching algorithm to the keypoints we 
mapped using the depth map data. Then we can score each algorithm based on how many of 
its keypoints fell within a certain distance threshold from the correctly mapped keypoints. 
A better score means that more of the keypoints generated by the matching algorithms were 
close to the ground truth keypoints from the depth map data.
"""
from copy import copy
import pandas as pd
from scipy.linalg import inv
import cv2
import numpy as np
from progressbar import ProgressBar
from Benchmarker import Benchmarker
import sys
sys.path.append("..")


def compare_matches(benchmarker, bundle, matches, query_image, train_image):
    """
    Compares the matches produced by the algorithm with the matches produced
    by the depth map data.

    Args: 
        bundle (Bundle): A Bundle object containing the data about the query and train images.
        matches (list): A list of UniMatch objects produced by the algorithm.
        query_image (numpy.ndarray): The query image.
        train_image (numpy.ndarray): The train image.

    Returns: A list of distances between the matches produced by the algorithm
    and the matches produced by the depth map data.
    """
    # Depth map of query image
    focal_length = bundle.query_image_intrinsics[0]
    offset_x = bundle.query_image_intrinsics[6]
    offset_y = bundle.query_image_intrinsics[7]

    query_depth_data = benchmarker.convert_depth_vectors(
        bundle.query_image_depth_map)
    train_depth_data = benchmarker.convert_depth_vectors(
        bundle.train_image_depth_map)

    # Actual depth feature points, with magnitude removed from the vector.
    query_depth_feature_points = np.array(
        (query_depth_data[0], -query_depth_data[1], -query_depth_data[2])
    ).T

    # calculate depths and pixels of feature points
    pixels = benchmarker.project_depth_onto_image(
        query_depth_feature_points, focal_length, offset_x, offset_y)

    final_query_image = benchmarker.plot_depth_map(
        bundle.query_image_depth_map, pixels, query_image)

    # reshape query and train pose matrices
    query_pose = np.array(bundle.query_image_pose).reshape(4, 4).T
    train_pose = np.array(bundle.train_image_pose).reshape(4, 4).T

    pose_difference = inv(query_pose) @ train_pose
    query_depth_data_projected_on_train = inv(
        pose_difference) @ query_depth_data

    projected_depth_feature_points = np.array(
        (
            query_depth_data_projected_on_train[0],
            -query_depth_data_projected_on_train[1],
            -query_depth_data_projected_on_train[2],
        )
    ).T

    pixels = []
    pixels = benchmarker.project_depth_onto_image(
        projected_depth_feature_points, focal_length, offset_x, offset_y)
    
    count = 0
    for pixel in pixels:
        if int(pixel[0]) in range(0, bundle.train_image.shape[0]) and int(pixel[1]) in range(0, bundle.train_image.shape[1]):
            count += 1

    print(
        f"count: {count}, pixels: {len(pixels)}, percentage: {count/len(pixels)}")

    final_train_image = benchmarker.plot_depth_map(
        bundle.query_image_depth_map, pixels, train_image)

    depth_point_to_algo_point_distances = []
    # Project corresponding query image keypoints onto train image which are matched using depth map data
    for unimatch in matches:
        matched_query_keypoint = (
            int(unimatch.queryPt.x), int(unimatch.queryPt.y))
        matched_train_keypoint = (
            int(unimatch.trainPt.x), int(unimatch.trainPt.y))

        # get corresponding depth map index for each keypoint.
        # Keypoints in a rectangular area around a depth index are matched to same index.
        # This is done since the resolution of depth map is lower than the resolution of the image.
        corresponding_depth_index = round(
            matched_query_keypoint[0] / 7.5
        ) * 192 + round(matched_query_keypoint[1] / 7.5)

        # Draw query image keypoints
        final_query_image = benchmarker.draw_circle(
            final_query_image, matched_query_keypoint, (0, 0, 0))

        algo_matched_point = np.array(
            (matched_train_keypoint[0], matched_train_keypoint[1]))
        depth_matched_point = np.array((int(pixels[corresponding_depth_index][0]), int(
            pixels[corresponding_depth_index][1])))

        # Draw train image keypoints, matched using the algorithm
        final_train_image = benchmarker.draw_circle(
            final_train_image, algo_matched_point, (0, 0, 0))

        # Plots corresponding depth point from query image on train image, matched using the depth data
        final_train_image = benchmarker.draw_circle(
            final_train_image, depth_matched_point, (255, 255, 255))

        # draw line between algo matched point and depth matched point
        final_train_image = cv2.line(
            final_train_image,
            algo_matched_point,
            depth_matched_point,
            (
                255,
                255,
                255,
            ),
            1,
        )

        depth_point_to_algo_point_distances.append(
            np.linalg.norm(algo_matched_point - depth_matched_point))
    cv2.imwrite("query.png", final_query_image)
    cv2.imwrite("train.png", final_train_image)
    # pause execution to view pairs of images
    _ = input("Press Enter to continue...")

    return depth_point_to_algo_point_distances, final_query_image, final_train_image


def benchmark(benchmarker):
    """
    Scores each algorithm based on how close their matches are with matches
    produced by the bundle data for each image pair.

    Args:
        benchmarker (Benchmarker): A Benchmarker object containing the data about the algorithms and sessions.

    Returns: A list of dictionaries containing information for each pair of images.
    """
    total_points_less_than_100 = []
    total_points = []
    correct_vs_incorrect_for_one_algo = []
    total_correct_for_one_algo = []
    output_data = []

    for algorithm in benchmarker.algorithms:
        for quantile in benchmarker.sweep_values:
            for session in benchmarker.sessions:
                print(repr(algorithm), session, quantile)
                pbar = ProgressBar()
                for bundle in pbar(session.bundles):
                    query_image = copy(bundle.query_image)
                    train_image = copy(bundle.train_image)

                    matches = algorithm.get_matches(
                        query_image, train_image, quantile, ratio_not_quantile=True)
                    # try:
                    distances, final_query_image, final_train_image = compare_matches(
                        benchmarker, bundle, matches, query_image, train_image)
                    out_dict = {
                        "bundle": bundle,
                        "matches": matches,
                        "distances": distances,
                        "algorithm": algorithm,
                        "quantile": quantile,
                    }
                    out_df = pd.DataFrame(out_dict)
                    output_data.append(out_df)
                    # print("distances", len(distances))

                    # find number of points that are less than 100 pixels away from the correct point
                    filtered_points = [x for x in distances if x < 100]
                    if len(distances) > 0:
                        total_points_less_than_100.append(
                            len(filtered_points))
                        total_points.append(len(distances))
            try:
                correct_vs_incorrect_for_one_algo.append(
                    (quantile, sum(total_points_less_than_100) / sum(total_points), algorithm))
                total_correct_for_one_algo.append(
                    sum(total_points_less_than_100))
                total_points_less_than_100 = []
                total_points = []
            except ZeroDivisionError:
                total_points_less_than_100 = []
                total_points = []
                # print("No matches found for", repr(algorithm), session, quantile)
                continue
    return output_data


def run_benchmark(algorithms, values):
    benchmarker = Benchmarker(algorithms, values)
    return benchmark(benchmarker)
